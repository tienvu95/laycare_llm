{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13469597,"sourceType":"datasetVersion","datasetId":8550546},{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Core libs for generation\n%pip install -U transformers accelerate bitsandbytes datasets --quiet\n# Metrics\n%pip install --no-deps -U rouge-score==0.1.2 nltk==3.9.1 tqdm==4.66.5\n%pip install --no-deps -U pytorch_pretrained_bert==0.6.2 moverscore==1.0.3\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:28:41.127465Z","iopub.execute_input":"2025-10-23T10:28:41.127705Z","iopub.status.idle":"2025-10-23T10:30:23.138009Z","shell.execute_reply.started":"2025-10-23T10:28:41.127682Z","shell.execute_reply":"2025-10-23T10:30:23.137150Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.3/506.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nCollecting rouge-score==0.1.2\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.11/dist-packages (3.9.1)\nCollecting tqdm==4.66.5\n  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ac4fa8aae57211a57744c344a1539dc98e7ef02c8c2a8c129956364ff76e82be\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge-score\nInstalling collected packages: tqdm, rouge-score\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.67.1\n    Uninstalling tqdm-4.67.1:\n      Successfully uninstalled tqdm-4.67.1\nSuccessfully installed rouge-score-0.1.2 tqdm-4.66.5\nNote: you may need to restart the kernel to use updated packages.\nCollecting pytorch_pretrained_bert==0.6.2\n  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl.metadata (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting moverscore==1.0.3\n  Downloading moverscore-1.0.3.tar.gz (7.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nDownloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: moverscore\n  Building wheel for moverscore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for moverscore: filename=moverscore-1.0.3-py3-none-any.whl size=7950 sha256=eb2a98ce4005bc8919a04bf298af239290f8cf442d1a05e7a2f9e5d477f7a604\n  Stored in directory: /root/.cache/pip/wheels/d5/56/44/23b9c85158529e8177d069f01d064e0ff2252c0d5b21608548\nSuccessfully built moverscore\nInstalling collected packages: pytorch_pretrained_bert, moverscore\nSuccessfully installed moverscore-1.0.3 pytorch_pretrained_bert-0.6.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%pip install -U --no-cache-dir \\\n  \"transformers==4.44.2\" \"accelerate==0.34.2\" \"bitsandbytes==0.43.1\" \\\n  \"rouge-score==0.1.2\" \"nltk==3.9.1\" \"tqdm==4.66.5\" \"safetensors>=0.4.3\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:30:58.437499Z","iopub.execute_input":"2025-10-23T12:30:58.437779Z","iopub.status.idle":"2025-10-23T12:31:13.795293Z","shell.execute_reply.started":"2025-10-23T12:30:58.437760Z","shell.execute_reply":"2025-10-23T12:31:13.794257Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\nCollecting bitsandbytes==0.43.1\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: rouge-score==0.1.2 in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: tqdm==4.66.5 in /usr/local/lib/python3.11/dist-packages (4.66.5)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (0.5.3)\nCollecting safetensors>=0.4.3\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (3.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.35.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2.32.5)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.44.2)\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (7.1.0)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (2.6.0+cu124)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score==0.1.2) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score==0.1.2) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (1.5.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.44.2) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2025.8.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.44.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.44.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.44.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.44.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.44.2) (2024.2.0)\nDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m329.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m226.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.8/485.8 kB\u001b[0m \u001b[31m348.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m191.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: safetensors, tokenizers, transformers, bitsandbytes, accelerate\n  Attempting uninstall: safetensors\n    Found existing installation: safetensors 0.5.3\n    Uninstalling safetensors-0.5.3:\n      Successfully uninstalled safetensors-0.5.3\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.22.1\n    Uninstalling tokenizers-0.22.1:\n      Successfully uninstalled tokenizers-0.22.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.57.1\n    Uninstalling transformers-4.57.1:\n      Successfully uninstalled transformers-4.57.1\n  Attempting uninstall: bitsandbytes\n    Found existing installation: bitsandbytes 0.48.1\n    Uninstalling bitsandbytes-0.48.1:\n      Successfully uninstalled bitsandbytes-0.48.1\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.11.0\n    Uninstalling accelerate-1.11.0:\n      Successfully uninstalled accelerate-1.11.0\nSuccessfully installed accelerate-0.34.2 bitsandbytes-0.43.1 safetensors-0.6.2 tokenizers-0.19.1 transformers-4.44.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os, gc, json, random, time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nimport sys, os\nsys.path.append(\"/kaggle/input/emnlp19-moverscore\")  # path that contains the 'moverscore_v2' folder\nfrom moverscore_v2 import get_idf_dict, word_mover_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:31:31.730395Z","iopub.execute_input":"2025-10-23T12:31:31.731248Z","iopub.status.idle":"2025-10-23T12:31:31.735836Z","shell.execute_reply.started":"2025-10-23T12:31:31.731217Z","shell.execute_reply":"2025-10-23T12:31:31.735030Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ====== SPEED PROFILE ======\nSPEED_PROFILE = os.getenv(\"SPEED_PROFILE\", \"balanced\").lower()\ndef preset(profile: str):\n    if profile == \"speed\":\n        return dict(MAX_INPUT_TOKENS=512, MAX_NEW_TOKENS=120, BATCH_TOKENS_BUDGET=16000, MAX_BATCH_SAMPLES=3, SAFETY=0.9)\n    if profile == \"safe\":\n        return dict(MAX_INPUT_TOKENS=768, MAX_NEW_TOKENS=128, BATCH_TOKENS_BUDGET=11000, MAX_BATCH_SAMPLES=2, SAFETY=0.85)\n    # balanced\n    return dict(MAX_INPUT_TOKENS=768, MAX_NEW_TOKENS=128, BATCH_TOKENS_BUDGET=14000, MAX_BATCH_SAMPLES=2, SAFETY=0.9)\n\n_P = preset(SPEED_PROFILE)\n\n# ====== CFG ======\nclass CFG:\n    LOCAL_MODEL_DIR = os.getenv(\"LLAMA_MODEL_DIR\", \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\")\n    MEDRED_CSV      = os.getenv(\"MEDRED_CSV\", \"/kaggle/input/medred/medredqa_test.csv\")\n    OUT_DIR = Path(os.getenv(\"OUT_DIR\", \"/kaggle/working\")); OUT_DIR.mkdir(parents=True, exist_ok=True)\n\n    # Generation (greedy)\n    MAX_NEW_TOKENS = int(os.getenv(\"MAX_NEW_TOKENS\", 80))  \n    DO_SAMPLE = False\n    MAX_BATCH_SAMPLES = int(os.getenv(\"MAX_BATCH_SAMPLES\", 3))\n\n    # Context / batching\n    MAX_INPUT_TOKENS    = int(os.getenv(\"MAX_INPUT_TOKENS\", 512))   # ↓ from 1024/768\n    BATCH_TOKENS_BUDGET = int(os.getenv(\"BATCH_TOKENS_BUDGET\", 18000))\n    SAFETY = float(os.getenv(\"BATCH_SAFETY\", 0.9))\n\n    SYSTEM_PROMPT = \"You are a careful medical assistant. Answer clearly and concisely for lay readers.\"\n    N_ROWS = int(os.getenv(\"N_ROWS\", 1000))           \n    SAMPLE_STRATEGY = os.getenv(\"SAMPLE_STRATEGY\", \"random\")  \n    SHARD_TOTAL = int(os.getenv(\"SHARD_TOTAL\", 1))    \n    SHARD_INDEX = int(os.getenv(\"SHARD_INDEX\", 0))    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:31:37.567812Z","iopub.execute_input":"2025-10-23T12:31:37.568439Z","iopub.status.idle":"2025-10-23T12:31:37.575757Z","shell.execute_reply.started":"2025-10-23T12:31:37.568415Z","shell.execute_reply":"2025-10-23T12:31:37.575131Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ====== perf knobs ======\nSEED = 42\nos.environ.setdefault(\"PYTHONHASHSEED\", str(SEED))\nrandom.seed(SEED); np.random.seed(SEED)\ntorch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.benchmark = True\ntry:\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\nexcept Exception:\n    pass\nos.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:128\")\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"; os.environ[\"JAX_PLATFORMS\"]=\"cpu\"; os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\"\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n\n# 4-bit quant\nbnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16)\ntok = AutoTokenizer.from_pretrained(CFG.LOCAL_MODEL_DIR, local_files_only=True, use_fast=True)\nif tok.pad_token is None: tok.pad_token = tok.eos_token\ntok.padding_side = \"left\"\n\n# prefer eager; flash if available\nmodel = AutoModelForCausalLM.from_pretrained(\n    CFG.LOCAL_MODEL_DIR,\n    device_map={\"\": \"cuda:0\"},\n    low_cpu_mem_usage=True,\n    quantization_config=bnb,\n    torch_dtype=torch.float16,\n    attn_implementation=\"eager\",\n    local_files_only=True,\n)\nmodel.eval()\nmodel.config.pad_token_id = tok.pad_token_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:30:48.814444Z","iopub.execute_input":"2025-10-23T10:30:48.814709Z","iopub.status.idle":"2025-10-23T10:34:47.790173Z","shell.execute_reply.started":"2025-10-23T10:30:48.814689Z","shell.execute_reply":"2025-10-23T10:34:47.789306Z"}},"outputs":[{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761215457.977483      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761215458.039873      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fed0ffd719647c9ab0cb04d0582829d"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# ========= batching =========\ndef make_batches_by_padded_cost(prompt_lens, budget_tokens, max_new, max_batch_samples, safety=0.9):\n    batches, cur = [], []\n    cur_max = 0\n    eff = int(budget_tokens * safety)\n    for i, L in enumerate(prompt_lens):\n        new_max = max(cur_max, L)\n        bs_next = len(cur) + 1\n        cost = bs_next * (new_max + max_new)\n        if cur and (cost > eff or len(cur) >= max_batch_samples):\n            batches.append(cur); cur = [i]; cur_max = L\n        else:\n            cur.append(i); cur_max = new_max\n    if cur: batches.append(cur)\n    return batches\n\n# ========= build prompts =========\ndef build_prompt(q: str) -> str:\n    msgs = [{\"role\":\"system\",\"content\":CFG.SYSTEM_PROMPT},{\"role\":\"user\",\"content\":q}]\n    try:\n        return tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n    except Exception:\n        return (\n            f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n{CFG.SYSTEM_PROMPT}\\n\"\n            f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n{q}\\n\"\n            f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n        )\n\n# ========= encode all once (CPU), track lens =========\n# df = pd.read_csv(CFG.MEDRED_CSV)\n# df = pd.DataFrame({\n#     \"id\": df[\"Unnamed: 0\"].astype(str),\n#     \"query\": (df[\"Title\"].fillna(\"\") + \"\\n\\n\" + df[\"Body\"].fillna(\"\")).str.strip(),\n#     \"reference\": df[\"Response\"].fillna(\"\").astype(str).str.strip()\n# })\n# df = df[df[\"query\"].str.len() > 0].reset_index(drop=True)\n\n# prompts = [build_prompt(q) for q in df[\"query\"].tolist()]\n\ndf_raw = pd.read_csv(CFG.MEDRED_CSV)\ndf = pd.DataFrame({\n    \"id\": df_raw.iloc[:, 0].astype(str),\n    \"query\": (df_raw[\"Title\"].fillna(\"\") + \"\\n\\n\" + df_raw[\"Body\"].fillna(\"\")).str.strip(),\n    \"reference\": df_raw[\"Response\"].fillna(\"\").astype(str).str.strip()\n})\ndf = df[df[\"query\"].str.len() > 0].reset_index(drop=True)\n\nif CFG.N_ROWS and CFG.N_ROWS > 0 and CFG.N_ROWS < len(df):\n    if CFG.SAMPLE_STRATEGY.lower() == \"head\":\n        df = df.iloc[:CFG.N_ROWS].copy()\n    else:\n        # random sample for better coverage; reproducible via SEED\n        df = df.sample(n=CFG.N_ROWS, random_state=SEED).sort_index().copy()\n        \nprompts = [build_prompt(q) for q in df[\"query\"].tolist()]\nassert len(prompts) == len(df), \"prompts/df length mismatch\"\n# IMPORTANT: get ragged LISTS, not tensors (avoids ValueError)\nenc = tok(\n    prompts,\n    add_special_tokens=True,\n    padding=False,                 # no global padding here\n    truncation=True,\n    max_length=CFG.MAX_INPUT_TOKENS,\n    return_tensors=None            # <- changed from \"pt\"\n)\n\ninput_ids_list = enc[\"input_ids\"]         # List[List[int]]\nlens = [len(x) for x in input_ids_list]   # lengths per sample\n\n# sort by length for tighter padding\norder = np.argsort(lens).tolist()\nids_sorted  = [df[\"id\"].iloc[i]        for i in order]\nref_sorted  = [df[\"reference\"].iloc[i] for i in order]\niids_sorted = [input_ids_list[i]       for i in order]\nlens_sorted = [lens[i]                 for i in order]\n\nbatches = make_batches_by_padded_cost(\n    lens_sorted,\n    CFG.BATCH_TOKENS_BUDGET,\n    CFG.MAX_NEW_TOKENS,\n    CFG.MAX_BATCH_SAMPLES,\n    safety=CFG.SAFETY\n)\n\n# multi-EOS for early stop (eot/eos and a newline fence)\neos_token_ids = [tok.eos_token_id]\ntry:\n    eot_id = tok.convert_tokens_to_ids(\"<|eot_id|>\")\n    if isinstance(eot_id, int) and eot_id != tok.eos_token_id:\n        eos_token_ids.append(eot_id)\nexcept Exception:\n    pass\nfor stop_str in [\"\\n\\n\", \"\\n###\", \"\\nUser:\", \"\\nAssistant:\"]:\n    ids = tok(stop_str, add_special_tokens=False).input_ids\n    if ids:\n        eos_token_ids.append(ids[-1])\n\n\ndef left_pad_collate(iid_list, device):\n    maxL = max(len(x) for x in iid_list)\n    pad_id = tok.pad_token_id\n    input_ids = torch.full((len(iid_list), maxL), pad_id, dtype=torch.long)\n    attn_mask = torch.zeros((len(iid_list), maxL), dtype=torch.bool)\n    for r, ids_r in enumerate(iid_list):\n        L = len(ids_r)\n        input_ids[r, -L:] = torch.as_tensor(ids_r, dtype=torch.long)\n        attn_mask[r,  -L:] = True\n    return input_ids.to(device, non_blocking=True), attn_mask.to(device, non_blocking=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:38:39.476062Z","iopub.execute_input":"2025-10-23T10:38:39.476712Z","iopub.status.idle":"2025-10-23T10:38:39.972472Z","shell.execute_reply.started":"2025-10-23T10:38:39.476687Z","shell.execute_reply":"2025-10-23T10:38:39.971867Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ========= generation (lean) =========\n\npred_rows = []\nwith torch.inference_mode():\n    for b_ix, idxs in enumerate(tqdm(batches, desc=f\"Generating[{SPEED_PROFILE}]\")):\n        batch_iids = [iids_sorted[i] for i in idxs]\n        input_ids, attn_mask = left_pad_collate(batch_iids, model.device)\n\n        out = model.generate(\n            input_ids=input_ids,\n            attention_mask=attn_mask,\n            max_new_tokens=CFG.MAX_NEW_TOKENS,  # paper: 150; using 96–128/150 for speed\n            do_sample=False,                      # greedy; sampling didn’t help MoverScore in paper\n            eos_token_id=eos_token_ids,\n            pad_token_id=tok.pad_token_id,\n            use_cache=True,\n        )\n\n        new_tokens = out[:, input_ids.shape[1]:]\n        texts = tok.batch_decode(new_tokens, skip_special_tokens=True)\n\n        for j, txt in zip(idxs, texts):\n            pred_rows.append({\"id\": str(ids_sorted[j]), \"prediction\": txt.strip()})\n\n        del input_ids, attn_mask, out, new_tokens, texts\n        torch.cuda.synchronize(); torch.cuda.empty_cache(); gc.collect()\n\npreds = pd.DataFrame(pred_rows)\npreds.to_csv(CFG.OUT_DIR / \"predictions_sub.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:38:44.262612Z","iopub.execute_input":"2025-10-23T10:38:44.263536Z","iopub.status.idle":"2025-10-23T12:15:23.308997Z","shell.execute_reply.started":"2025-10-23T10:38:44.263508Z","shell.execute_reply":"2025-10-23T12:15:23.308205Z"}},"outputs":[{"name":"stderr","text":"Generating[balanced]:   0%|          | 0/334 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nGenerating[balanced]: 100%|██████████| 334/334 [1:36:39<00:00, 17.36s/it]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ==== Compatibility shim for moverscore_v2 ====\ndef _patch_tokenizer_max_len():\n    try:\n        import transformers\n        for name in (\"PreTrainedTokenizer\", \"PreTrainedTokenizerFast\"):\n            cls = getattr(transformers, name, None)\n            if cls is not None and not hasattr(cls, \"max_len\"):\n                cls.max_len = property(lambda self: getattr(self, \"model_max_length\", 512))\n    except Exception:\n        pass\n_patch_tokenizer_max_len()\n\n# ==== END shim ====\n\n# ========= evaluation: fast ROUGE + faster MoverScore =========\ndef normalize(s): return \" \".join((s or \"\").split())\neval_df = df[[\"id\",\"reference\"]].merge(preds, on=\"id\", how=\"left\")\neval_df[\"reference\"]  = eval_df[\"reference\"].map(normalize)\neval_df[\"prediction\"] = eval_df[\"prediction\"].fillna(\"\").map(normalize)\n\n# ROUGE-1 (parallel)\nfrom rouge_score import rouge_scorer\nfrom multiprocessing import Pool, cpu_count\ndef _init_scorer():\n    global _SCORER\n    _SCORER = rouge_scorer.RougeScorer([\"rouge1\"], use_stemmer=True)\ndef _r1_pair(args):\n    ref, hyp = args\n    return _SCORER.score(ref, hyp)[\"rouge1\"].fmeasure\npairs = list(zip(eval_df[\"reference\"], eval_df[\"prediction\"]))\nwith Pool(processes=max(1, cpu_count()-1), initializer=_init_scorer) as pool:\n    r1 = np.array(list(tqdm(pool.imap(_r1_pair, pairs), total=len(eval_df), desc=\"ROUGE-1(F1)\")), dtype=np.float32)\n\n# MoverScore v2 (now works)\nDEV_SKIP_MOVERSCORE = bool(int(os.getenv(\"DEV_SKIP_MOVERSCORE\", \"0\")))\ntry:\n    if DEV_SKIP_MOVERSCORE:\n        raise RuntimeError(\"DEV_SKIP_MOVERSCORE=1\")\n    from moverscore_v2 import get_idf_dict, word_mover_score\n\n    # OPTIONAL: only score a subset to speed up (comment this block to score all)\n    MS_MAX = int(os.getenv(\"MS_MAX\", \"0\"))  # e.g., set 200 to score first 200\n    eval_df_ms = eval_df if MS_MAX <= 0 or MS_MAX >= len(eval_df) else eval_df.iloc[:MS_MAX].copy()\n\n    refs = eval_df_ms[\"reference\"].tolist(); hyps = eval_df_ms[\"prediction\"].tolist()\n    idf_ref = get_idf_dict(refs); idf_hyp = get_idf_dict(hyps)  # compute once\n    ms_list = word_mover_score(\n        refs, hyps, idf_ref, idf_hyp,\n        stop_words=None, n_gram=1, remove_subwords=True, batch_size=64\n    )\n    ms = np.array([float(x) for x in ms_list], dtype=np.float32)\n\n    # If subset scored, broadcast back with NaNs elsewhere\n    if len(eval_df_ms) != len(eval_df):\n        ms_full = np.full(len(eval_df), np.nan, dtype=np.float32)\n        ms_full[:len(ms)] = ms\n        ms = ms_full\n    moverscore_ok = True\nexcept Exception as e:\n    print(\"WARNING: MoverScore failed:\", e)\n    ms = np.full(len(eval_df), np.nan, dtype=np.float32)\n    moverscore_ok = False\n\neval_df[\"rouge1_f1\"] = r1\neval_df[\"moverscore\"] = ms\nsummary = {\n    \"n\": int(len(eval_df)),\n    \"rouge1_f1_mean\": float(np.nanmean(r1)),\n    \"rouge1_f1_std\":  float(np.nanstd(r1)),\n    \"moverscore_mean\": float(np.nanmean(ms)),\n    \"moverscore_std\":  float(np.nanstd(ms)),\n    \"moverscore_ok\": moverscore_ok,\n    \"speed_profile\": SPEED_PROFILE,\n    \"ms_max\": int(os.getenv(\"MS_MAX\", \"0\")),\n    \"dev_skip_moverscore\": DEV_SKIP_MOVERSCORE,\n}\nwith open(CFG.OUT_DIR/\"summary.json\", \"w\") as f: json.dump(summary, f, indent=2)\neval_df.to_csv(CFG.OUT_DIR/\"per_example_scores.csv\", index=False)\nprint(\"[DONE]\", json.dumps(summary, indent=2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T12:32:12.708965Z","iopub.execute_input":"2025-10-23T12:32:12.709291Z","iopub.status.idle":"2025-10-23T12:32:15.041130Z","shell.execute_reply.started":"2025-10-23T12:32:12.709264Z","shell.execute_reply":"2025-10-23T12:32:15.040154Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nROUGE-1(F1): 100%|██████████| 1002/1002 [00:00<00:00, 1450.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"WARNING: MoverScore failed: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not str\n[DONE] {\n  \"n\": 1002,\n  \"rouge1_f1_mean\": 0.16458486020565033,\n  \"rouge1_f1_std\": 0.08978775888681412,\n  \"moverscore_mean\": NaN,\n  \"moverscore_std\": NaN,\n  \"moverscore_ok\": false,\n  \"speed_profile\": \"balanced\",\n  \"ms_max\": 0,\n  \"dev_skip_moverscore\": false\n}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/566072962.py:70: RuntimeWarning: Mean of empty slice\n  \"moverscore_mean\": float(np.nanmean(ms)),\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n","output_type":"stream"}],"execution_count":17}]}